{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mFq2aRw_w3cL"
      },
      "source": [
        "##### Copyright 2020 The TensorFlow Quantum Authors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eOzjTj_JxBnv"
      },
      "outputs": [],
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lusn46uoyCcv"
      },
      "source": [
        "[link text](https://)# Binary classification of quantum states\n",
        "\n",
        "Initial Tutorial Author : Antonio J. Martinez\n",
        "\n",
        "Initial Tutorial Contributors : Masoud Mohseni\n",
        "\n",
        "Initial Tutorial Created : 2020-Feb-14\n",
        "\n",
        "Initial Tutorial Last updated : 2020-Feb-29\n",
        "\n",
        "---\n",
        "\n",
        "Current Experiment Author : Anneliese Brei\n",
        "\n",
        "Current Experiment Created : 2022-Jan-3\n",
        "\n",
        "Current Experiment Last updated : 2022-Jan-3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O8hXbFbkv_D_"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/tensorflow/quantum/blob/research/binary_classifier/binary_classifier.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2j5_tMNP12Mq"
      },
      "source": [
        "An elementary learning task is [binary classification](https://en.wikipedia.org/wiki/Binary_classification), a supervised task in which the learner is to distinguish which of two classes a given datapoint has been drawn from.  Here, using ideas from the paper [Universal discriminative quantum neural networks](https://arxiv.org/abs/1805.08654) in the one-qubit setting, we train a hybrid quantum-classical neural network to distinguish between quantum data sources."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IrWw_xv4fs44"
      },
      "source": [
        "## Import dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rFqxhKypZoSJ",
        "outputId": "fa979094-0413-44ac-a194-9ceb9255cf02"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.7/dist-packages (2.8.0)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.17.3)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=9.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (13.0.0)\n",
            "Collecting tf-estimator-nightly==2.8.0.dev2021122109\n",
            "  Downloading tf_estimator_nightly-2.8.0.dev2021122109-py2.py3-none-any.whl (462 kB)\n",
            "\u001b[K     |████████████████████████████████| 462 kB 4.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.13.3)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.44.0)\n",
            "Requirement already satisfied: absl-py>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.0.0)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.21.5)\n",
            "Requirement already satisfied: flatbuffers>=1.12 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: tensorboard<2.9,>=2.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.8.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.1.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.15.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.10.0.2)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.1.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.1.2)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.24.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: gast>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.5.3)\n",
            "Requirement already satisfied: keras<2.9,>=2.8.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.8.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from tensorflow) (57.4.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.7/dist-packages (from astunparse>=1.6.0->tensorflow) (0.37.1)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py>=2.9.0->tensorflow) (1.5.2)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow) (0.4.6)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow) (1.0.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow) (2.23.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow) (0.6.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow) (3.3.6)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow) (1.35.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow) (1.8.1)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow) (4.2.4)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow) (4.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow) (4.11.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow) (3.7.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow) (0.4.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (1.24.3)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow) (3.2.0)\n",
            "Installing collected packages: tf-estimator-nightly\n",
            "Successfully installed tf-estimator-nightly-2.8.0.dev2021122109\n",
            "Collecting qutip\n",
            "  Downloading qutip-4.6.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (14.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 14.8 MB 238 kB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.16.6 in /usr/local/lib/python3.7/dist-packages (from qutip) (1.21.5)\n",
            "Requirement already satisfied: scipy>=1.0 in /usr/local/lib/python3.7/dist-packages (from qutip) (1.4.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from qutip) (21.3)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->qutip) (3.0.7)\n",
            "Installing collected packages: qutip\n",
            "Successfully installed qutip-4.6.3\n"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade tensorflow\n",
        "!pip install qutip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "xcDb1zbSdXKi",
        "outputId": "d1351267-a88a-4847-f0b2-a3610707c4a2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorflow-quantum\n",
            "  Downloading tensorflow_quantum-0.6.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (10.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 10.5 MB 4.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: protobuf==3.17.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow-quantum) (3.17.3)\n",
            "Collecting google-auth==1.18.0\n",
            "  Downloading google_auth-1.18.0-py2.py3-none-any.whl (90 kB)\n",
            "\u001b[K     |████████████████████████████████| 90 kB 7.3 MB/s \n",
            "\u001b[?25hCollecting cirq-core>=0.13.1\n",
            "  Downloading cirq_core-0.13.1-py3-none-any.whl (1.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.6 MB 7.1 MB/s \n",
            "\u001b[?25hCollecting sympy==1.8\n",
            "  Downloading sympy-1.8-py3-none-any.whl (6.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.1 MB 29.7 MB/s \n",
            "\u001b[?25hCollecting googleapis-common-protos==1.52.0\n",
            "  Downloading googleapis_common_protos-1.52.0-py2.py3-none-any.whl (100 kB)\n",
            "\u001b[K     |████████████████████████████████| 100 kB 7.4 MB/s \n",
            "\u001b[?25hCollecting google-api-core==1.21.0\n",
            "  Downloading google_api_core-1.21.0-py2.py3-none-any.whl (90 kB)\n",
            "\u001b[K     |████████████████████████████████| 90 kB 7.7 MB/s \n",
            "\u001b[?25hCollecting cirq-google>=0.13.1\n",
            "  Downloading cirq_google-0.13.1-py3-none-any.whl (437 kB)\n",
            "\u001b[K     |████████████████████████████████| 437 kB 43.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pytz in /usr/local/lib/python3.7/dist-packages (from google-api-core==1.21.0->tensorflow-quantum) (2018.9)\n",
            "Requirement already satisfied: setuptools>=34.0.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core==1.21.0->tensorflow-quantum) (57.4.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core==1.21.0->tensorflow-quantum) (1.15.0)\n",
            "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core==1.21.0->tensorflow-quantum) (2.23.0)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth==1.18.0->tensorflow-quantum) (4.2.4)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth==1.18.0->tensorflow-quantum) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth==1.18.0->tensorflow-quantum) (4.8)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.7/dist-packages (from sympy==1.8->tensorflow-quantum) (1.2.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from cirq-core>=0.13.1->tensorflow-quantum) (3.10.0.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from cirq-core>=0.13.1->tensorflow-quantum) (1.4.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from cirq-core>=0.13.1->tensorflow-quantum) (4.63.0)\n",
            "Requirement already satisfied: sortedcontainers~=2.0 in /usr/local/lib/python3.7/dist-packages (from cirq-core>=0.13.1->tensorflow-quantum) (2.4.0)\n",
            "Requirement already satisfied: numpy~=1.16 in /usr/local/lib/python3.7/dist-packages (from cirq-core>=0.13.1->tensorflow-quantum) (1.21.5)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from cirq-core>=0.13.1->tensorflow-quantum) (1.3.5)\n",
            "Requirement already satisfied: matplotlib~=3.0 in /usr/local/lib/python3.7/dist-packages (from cirq-core>=0.13.1->tensorflow-quantum) (3.2.2)\n",
            "Collecting duet~=0.2.0\n",
            "  Downloading duet-0.2.5-py3-none-any.whl (28 kB)\n",
            "Requirement already satisfied: networkx~=2.4 in /usr/local/lib/python3.7/dist-packages (from cirq-core>=0.13.1->tensorflow-quantum) (2.6.3)\n",
            "Requirement already satisfied: google-api-core[grpc]<2.0.0dev,>=1.14.0 in /usr/local/lib/python3.7/dist-packages (from cirq-google>=0.13.1->tensorflow-quantum) (1.26.3)\n",
            "Collecting typing-extensions\n",
            "  Downloading typing_extensions-3.10.0.0-py3-none-any.whl (26 kB)\n",
            "Collecting google-api-core[grpc]<2.0.0dev,>=1.14.0\n",
            "  Downloading google_api_core-1.31.5-py2.py3-none-any.whl (93 kB)\n",
            "\u001b[K     |████████████████████████████████| 93 kB 1.2 MB/s \n",
            "\u001b[?25h  Downloading google_api_core-1.31.4-py2.py3-none-any.whl (93 kB)\n",
            "\u001b[K     |████████████████████████████████| 93 kB 882 kB/s \n",
            "\u001b[?25h  Downloading google_api_core-1.31.3-py2.py3-none-any.whl (93 kB)\n",
            "\u001b[K     |████████████████████████████████| 93 kB 643 kB/s \n",
            "\u001b[?25h  Downloading google_api_core-1.31.2-py2.py3-none-any.whl (93 kB)\n",
            "\u001b[K     |████████████████████████████████| 93 kB 1.1 MB/s \n",
            "\u001b[?25h  Downloading google_api_core-1.31.1-py2.py3-none-any.whl (93 kB)\n",
            "\u001b[K     |████████████████████████████████| 93 kB 858 kB/s \n",
            "\u001b[?25h  Downloading google_api_core-1.31.0-py2.py3-none-any.whl (93 kB)\n",
            "\u001b[K     |████████████████████████████████| 93 kB 378 kB/s \n",
            "\u001b[?25h  Downloading google_api_core-1.30.0-py2.py3-none-any.whl (93 kB)\n",
            "\u001b[K     |████████████████████████████████| 93 kB 1.1 MB/s \n",
            "\u001b[?25h  Downloading google_api_core-1.29.0-py2.py3-none-any.whl (93 kB)\n",
            "\u001b[K     |████████████████████████████████| 93 kB 472 kB/s \n",
            "\u001b[?25h  Downloading google_api_core-1.28.0-py2.py3-none-any.whl (92 kB)\n",
            "\u001b[K     |████████████████████████████████| 92 kB 510 kB/s \n",
            "\u001b[?25h  Downloading google_api_core-1.27.0-py2.py3-none-any.whl (93 kB)\n",
            "\u001b[K     |████████████████████████████████| 93 kB 723 kB/s \n",
            "\u001b[?25h  Downloading google_api_core-1.26.2-py2.py3-none-any.whl (93 kB)\n",
            "\u001b[K     |████████████████████████████████| 93 kB 621 kB/s \n",
            "\u001b[?25h  Downloading google_api_core-1.26.1-py2.py3-none-any.whl (92 kB)\n",
            "\u001b[K     |████████████████████████████████| 92 kB 930 kB/s \n",
            "\u001b[?25h  Downloading google_api_core-1.26.0-py2.py3-none-any.whl (92 kB)\n",
            "\u001b[K     |████████████████████████████████| 92 kB 817 kB/s \n",
            "\u001b[?25h  Downloading google_api_core-1.25.1-py2.py3-none-any.whl (92 kB)\n",
            "\u001b[K     |████████████████████████████████| 92 kB 186 kB/s \n",
            "\u001b[?25h  Downloading google_api_core-1.25.0-py2.py3-none-any.whl (92 kB)\n",
            "\u001b[K     |████████████████████████████████| 92 kB 134 kB/s \n",
            "\u001b[?25h  Downloading google_api_core-1.24.1-py2.py3-none-any.whl (92 kB)\n",
            "\u001b[K     |████████████████████████████████| 92 kB 6.8 MB/s \n",
            "\u001b[?25h  Downloading google_api_core-1.24.0-py2.py3-none-any.whl (91 kB)\n",
            "\u001b[K     |████████████████████████████████| 91 kB 7.9 MB/s \n",
            "\u001b[?25h  Downloading google_api_core-1.23.0-py2.py3-none-any.whl (91 kB)\n",
            "\u001b[K     |████████████████████████████████| 91 kB 8.0 MB/s \n",
            "\u001b[?25h  Downloading google_api_core-1.22.4-py2.py3-none-any.whl (91 kB)\n",
            "\u001b[K     |████████████████████████████████| 91 kB 7.7 MB/s \n",
            "\u001b[?25h  Downloading google_api_core-1.22.3-py2.py3-none-any.whl (91 kB)\n",
            "\u001b[K     |████████████████████████████████| 91 kB 7.1 MB/s \n",
            "\u001b[?25h  Downloading google_api_core-1.22.2-py2.py3-none-any.whl (91 kB)\n",
            "\u001b[K     |████████████████████████████████| 91 kB 4.5 MB/s \n",
            "\u001b[?25h  Downloading google_api_core-1.22.1-py2.py3-none-any.whl (91 kB)\n",
            "\u001b[K     |████████████████████████████████| 91 kB 7.4 MB/s \n",
            "\u001b[?25h  Downloading google_api_core-1.22.0-py2.py3-none-any.whl (91 kB)\n",
            "\u001b[K     |████████████████████████████████| 91 kB 5.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: grpcio<2.0dev,>=1.29.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core==1.21.0->tensorflow-quantum) (1.44.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib~=3.0->cirq-core>=0.13.1->tensorflow-quantum) (1.3.2)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib~=3.0->cirq-core>=0.13.1->tensorflow-quantum) (2.8.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib~=3.0->cirq-core>=0.13.1->tensorflow-quantum) (0.11.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib~=3.0->cirq-core>=0.13.1->tensorflow-quantum) (3.0.7)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth==1.18.0->tensorflow-quantum) (0.4.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core==1.21.0->tensorflow-quantum) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core==1.21.0->tensorflow-quantum) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core==1.21.0->tensorflow-quantum) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core==1.21.0->tensorflow-quantum) (1.24.3)\n",
            "Installing collected packages: typing-extensions, googleapis-common-protos, google-auth, sympy, google-api-core, duet, cirq-core, cirq-google, tensorflow-quantum\n",
            "  Attempting uninstall: typing-extensions\n",
            "    Found existing installation: typing-extensions 3.10.0.2\n",
            "    Uninstalling typing-extensions-3.10.0.2:\n",
            "      Successfully uninstalled typing-extensions-3.10.0.2\n",
            "  Attempting uninstall: googleapis-common-protos\n",
            "    Found existing installation: googleapis-common-protos 1.55.0\n",
            "    Uninstalling googleapis-common-protos-1.55.0:\n",
            "      Successfully uninstalled googleapis-common-protos-1.55.0\n",
            "  Attempting uninstall: google-auth\n",
            "    Found existing installation: google-auth 1.35.0\n",
            "    Uninstalling google-auth-1.35.0:\n",
            "      Successfully uninstalled google-auth-1.35.0\n",
            "  Attempting uninstall: sympy\n",
            "    Found existing installation: sympy 1.7.1\n",
            "    Uninstalling sympy-1.7.1:\n",
            "      Successfully uninstalled sympy-1.7.1\n",
            "  Attempting uninstall: google-api-core\n",
            "    Found existing installation: google-api-core 1.26.3\n",
            "    Uninstalling google-api-core-1.26.3:\n",
            "      Successfully uninstalled google-api-core-1.26.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "pydata-google-auth 1.3.0 requires google-auth<3.0dev,>=1.25.0; python_version >= \"3.6\", but you have google-auth 1.18.0 which is incompatible.\u001b[0m\n",
            "Successfully installed cirq-core-0.13.1 cirq-google-0.13.1 duet-0.2.5 google-api-core-1.21.0 google-auth-1.18.0 googleapis-common-protos-1.52.0 sympy-1.8 tensorflow-quantum-0.6.1 typing-extensions-3.10.0.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google"
                ]
              }
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "!pip install tensorflow-quantum"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kW2sb1rAfhwt"
      },
      "outputs": [],
      "source": [
        "import cirq\n",
        "import numpy as np\n",
        "import qutip\n",
        "import random\n",
        "import sympy\n",
        "import tensorflow as tf\n",
        "import tensorflow_quantum as tfq\n",
        "\n",
        "# visualization tools\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "from cirq.contrib.svg import SVGCircuit"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hd1mo09k1Dt3"
      },
      "source": [
        "## Quantum dataset\n",
        "For our quantum dataset, you will generate two blobs on the surface of the Bloch sphere.  The task will be to learn a model to distinguish members of these blobs.  To do this, you first select two axes in the X-Z plane of the block sphere, then select random points uniformly distributed around them:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FUEawr8o1C2g"
      },
      "outputs": [],
      "source": [
        "def generate_dataset(qubit, theta_a, theta_b, num_samples):\n",
        "  \"\"\"Generate a dataset of points on `qubit` near the two given angles; labels\n",
        "  for the two clusters use a one-hot encoding.\n",
        "  \"\"\"\n",
        "  q_data = []\n",
        "  bloch = {\"a\": [[], [], []], \"b\": [[], [], []]}\n",
        "  labels = []\n",
        "  blob_size = abs(theta_a - theta_b) / 5\n",
        "  for _ in range(num_samples):\n",
        "    coin = random.random()\n",
        "    spread_x = np.random.uniform(-blob_size, blob_size)\n",
        "    spread_y = np.random.uniform(-blob_size, blob_size)\n",
        "    if coin < 0.5:\n",
        "      label = [1, 0]\n",
        "      angle = theta_a + spread_y\n",
        "      source = \"a\"\n",
        "    else:\n",
        "      label = [0, 1]\n",
        "      angle = theta_b + spread_y\n",
        "      source = \"b\"\n",
        "    labels.append(label)\n",
        "    q_data.append(cirq.Circuit(cirq.ry(-angle)(qubit), cirq.rx(-spread_x)(qubit)))\n",
        "    bloch[source][0].append(np.cos(angle))\n",
        "    bloch[source][1].append(np.sin(angle)*np.sin(spread_x))\n",
        "    bloch[source][2].append(np.sin(angle)*np.cos(spread_x))\n",
        "  return tfq.convert_to_tensor(q_data), np.array(labels), bloch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YTOqrbFzgTq9"
      },
      "outputs": [],
      "source": [
        "def build_model(theta_a, theta_b):\n",
        "\n",
        "  qubit = cirq.GridQubit(0, 0)\n",
        "\n",
        "  # Build the quantum model layer 1\n",
        "  theta = sympy.Symbol('theta')\n",
        "  q_model = cirq.Circuit(cirq.ry(theta)(qubit))\n",
        "  q_data_input = tf.keras.Input(\n",
        "      shape=(), dtype=tf.dtypes.string)\n",
        "  expectation = tfq.layers.PQC(q_model, cirq.Z(qubit))\n",
        "  expectation_output = expectation(q_data_input)\n",
        "\n",
        "  # Attach the classical SoftMax classifier\n",
        "  classifier = tf.keras.layers.Dense(2, activation=tf.keras.activations.softmax)\n",
        "  classifier_output = classifier(expectation_output)\n",
        "  model = tf.keras.Model(inputs=q_data_input, outputs=classifier_output)\n",
        "\n",
        "  # Standard compilation for classification\n",
        "  model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.1),\n",
        "                loss=tf.keras.losses.CategoricalCrossentropy(),\n",
        "                metrics=['accuracy'])\n",
        "  \n",
        "  return model, qubit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_oBfIyntWT_I"
      },
      "outputs": [],
      "source": [
        "# Constants\n",
        "theta_a = 1\n",
        "theta_b = 4\n",
        "\n",
        "num_samples = 5000 # Number datapoints\n",
        "iterations = 10\n",
        "num_epochs = 5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G6UTLOpYlLIu"
      },
      "source": [
        "## 1-Node Framework\n",
        "Implement Binary Classification in a federated learning framework using 2 separate models run by 2 different simulators. Model 0 represents the global model. Model 1 is a client. The parameters are averaged and used to update the global model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x5a9iurClio6"
      },
      "outputs": [],
      "source": [
        "def run_1_node():\n",
        "\n",
        "  model0, _ = build_model(theta_a, theta_b) # global\n",
        "  model1, _ = build_model(theta_a, theta_b) # client\n",
        "\n",
        "  loss = []       # Loss value from each iteration\n",
        "  accuracy = []   # Accuracy % of each iteration\n",
        "\n",
        "  for round in range(iterations):\n",
        "\n",
        "    # Generate new local data for client\n",
        "    qubit = cirq.GridQubit(0, 0)\n",
        "    q_data1, labels1, _ = generate_dataset(qubit, theta_a, theta_b, num_samples) \n",
        "\n",
        "    # Train client 1 with (all) local data\n",
        "    history1 = model1.fit(x=q_data1, y=labels1, epochs=num_epochs, verbose=0)\n",
        "\n",
        "    weights0 = model0.get_weights()   # Extract global weights (parameters)\n",
        "    weights1 = model1.get_weights()   # Extract client weights (parameters)\n",
        "\n",
        "    # Average weights\n",
        "    avg_weights = weights0\n",
        "    for j in range(len(weights0)):\n",
        "      avg_weights[j] = (weights0[j] + weights1[j]) / 2\n",
        "    \n",
        "    #print(\"Averaged weights: \", avg_weights)  \n",
        "\n",
        "    # Update weights\n",
        "    model0.set_weights(avg_weights)\n",
        "    model1.set_weights(avg_weights)\n",
        "\n",
        "    # Test updated global model for metrics\n",
        "    qubit = cirq.GridQubit(0, 0)\n",
        "    test_data, test_labels, test_bloch_p = generate_dataset(qubit, theta_a, theta_b, 500)\n",
        "    test_results = model0.evaluate(test_data, test_labels, verbose=0)\n",
        "\n",
        "    loss.append(test_results[0])        # Save loss value of each round\n",
        "    accuracy.append(test_results[1])    # Save accuracy of each round\n",
        "\n",
        "  return loss, accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0W-D3bcjfd-C"
      },
      "source": [
        "## 2-Node Framework\n",
        "Implement Binary Classification in a federated learning framework using 3 client nodes and the global node models run by 3 different simulators. Model 0 represents the global model. Models 1 and 2 are clients. The parameters are averaged and used to update the global model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XfAzDNnWgVC0"
      },
      "outputs": [],
      "source": [
        "def run_2_node():\n",
        "    \n",
        "  model0, _ = build_model(theta_a, theta_b) #global\n",
        "  model1, _ = build_model(theta_a, theta_b) #client1\n",
        "  model2, _ = build_model(theta_a, theta_b) #client2\n",
        "\n",
        "  loss = []       # Loss value from each iteration\n",
        "  accuracy = []   # Accuracy % of each iteration\n",
        "\n",
        "  for round in range(iterations):\n",
        "\n",
        "    # Generate new local data for clients, D2 and D3\n",
        "    qubit = cirq.GridQubit(0, 0)\n",
        "    q_data, labels, _ = generate_dataset(qubit, theta_a, theta_b, num_samples) \n",
        "    \n",
        "    q_data1 = q_data[:2500]\n",
        "    q_data2 = q_data[2500:]\n",
        "    labels1 = labels[:2500]\n",
        "    labels2 = labels[2500:]\n",
        "\n",
        "    # Train clients with local data\n",
        "    history1 = model1.fit(x=q_data1, y=labels1, epochs=num_epochs, verbose=0)\n",
        "    history2 = model2.fit(x=q_data2, y=labels2, epochs=num_epochs, verbose=0)\n",
        "\n",
        "    weights0 = model0.get_weights()   # Extract global weights (parameters)\n",
        "    weights1 = model1.get_weights()   # Extract client weights (parameters)\n",
        "    weights2 = model2.get_weights()   # Extract client weights (parameters)\n",
        "\n",
        "    # Average weights\n",
        "    avg_weights = weights0\n",
        "    for j in range(len(weights0)):\n",
        "      avg_weights[j] = (weights0[j] + weights1[j] + weights2[j]) / 3 \n",
        "\n",
        "    # Update weights\n",
        "    model0.set_weights(avg_weights)\n",
        "    model1.set_weights(avg_weights)\n",
        "    model2.set_weights(avg_weights)\n",
        "\n",
        "    # Test updated global model for metrics\n",
        "    qubit = cirq.GridQubit(0, 0)\n",
        "    test_data, test_labels, test_bloch_p = generate_dataset(qubit, theta_a, theta_b, 500)\n",
        "    test_results = model0.evaluate(test_data,test_labels, verbose=0)\n",
        "\n",
        "    loss.append(test_results[0])        # Save loss value of each round\n",
        "    accuracy.append(test_results[1])    # Save accuracy of each round\n",
        "\n",
        "  return loss, accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kNs6hRg0uJBw"
      },
      "source": [
        "## 5-Node Framework\n",
        "Implement Binary Classification in a federated learning framework using 5 separate models run by 5 different simulators. Model 0 represents the global model. Models 1, 2, 3, 4 are clients. The parameters are averaged and used to update the global model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1F4DH01qr_jg"
      },
      "outputs": [],
      "source": [
        "def run_5_node():\n",
        "\n",
        "  # Create 5-node framework: 1 global model, 4 clients\n",
        "  model0, _ = build_model(theta_a, theta_b) #global\n",
        "  model1, _ = build_model(theta_a, theta_b) #client 1\n",
        "  model2, _ = build_model(theta_a, theta_b) #client 2\n",
        "  model3, _ = build_model(theta_a, theta_b) #client 3\n",
        "  model4, _ = build_model(theta_a, theta_b) #client 4\n",
        "  model5, _ = build_model(theta_a, theta_b) #client 5\n",
        "\n",
        "  loss = []       # Loss value from each iteration\n",
        "  accuracy = []   # Accuracy % of each iteration\n",
        "\n",
        "  for round in range(iterations):\n",
        "\n",
        "    # Generate new local data for clients, D1, D2, D3, D4\n",
        "    qubit = cirq.GridQubit(0, 0)\n",
        "    q_data, labels, _ = generate_dataset(qubit, theta_a, theta_b, num_samples)\n",
        "\n",
        "    q_data1 = q_data[:1000]\n",
        "    q_data2 = q_data[1000:2000]\n",
        "    q_data3 = q_data[2000:3000]\n",
        "    q_data4 = q_data[3000:4000]\n",
        "    q_data5 = q_data[4000:]\n",
        "\n",
        "    labels1 = labels[:1000]\n",
        "    labels2 = labels[1000:2000]\n",
        "    labels3 = labels[2000:3000]\n",
        "    labels4 = labels[3000:4000]\n",
        "    labels5 = labels[4000:] \n",
        "\n",
        "    # Train clients with local data\n",
        "    history1 = model1.fit(x=q_data1, y=labels1, epochs=num_epochs, verbose=0)\n",
        "    history2 = model2.fit(x=q_data2, y=labels2, epochs=num_epochs, verbose=0)\n",
        "    history3 = model3.fit(x=q_data3, y=labels3, epochs=num_epochs, verbose=0)\n",
        "    history4 = model4.fit(x=q_data4, y=labels4, epochs=num_epochs, verbose=0)\n",
        "    history5 = model5.fit(x=q_data5, y=labels5, epochs=num_epochs, verbose=0)\n",
        "\n",
        "    weights0 = model0.get_weights()   # Extract global weights (parameters)\n",
        "    weights1 = model1.get_weights()   # Extract client weights (parameters)\n",
        "    weights2 = model2.get_weights()   # Extract client weights (parameters)\n",
        "    weights3 = model3.get_weights()   # Extract client weights (parameters)\n",
        "    weights4 = model4.get_weights()   # Extract client weights (parameters)\n",
        "    weights5 = model5.get_weights()   # Extract client weights (parameters)\n",
        "\n",
        "    # Average weights\n",
        "    avg_weights = weights0\n",
        "    for j in range(len(weights0)):\n",
        "      avg_weights[j] = (weights0[j] + weights1[j] + weights2[j] + weights3[j] + weights4[j] + weights5[j]) / 6\n",
        "\n",
        "    # Update weights\n",
        "    model0.set_weights(avg_weights)\n",
        "    model1.set_weights(avg_weights)\n",
        "    model2.set_weights(avg_weights)\n",
        "    model3.set_weights(avg_weights)\n",
        "    model4.set_weights(avg_weights)\n",
        "    model5.set_weights(avg_weights)\n",
        "\n",
        "    # Test updated global model for metrics\n",
        "    qubit = cirq.GridQubit(0, 0)\n",
        "    test_data, test_labels, test_bloch_p = generate_dataset(qubit, theta_a, theta_b, 500)\n",
        "    test_results = model0.evaluate(test_data,test_labels, verbose=0)\n",
        "\n",
        "    loss.append(test_results[0])        # Save loss value of each round\n",
        "    accuracy.append(test_results[1])    # Save accuracy of each round\n",
        "\n",
        "  return loss, accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hlgJmNuOemsi"
      },
      "source": [
        "## 10-Node Framework\n",
        "Implement Binary Classification in a federated learning framework using 10 clients. Model 0 represents the global model. Models 1, 2, 3, 4, 5, 6, 7, 8, 9, 10 are clients. The parameters are averaged and used to update the global model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BPVIqxysfBWv"
      },
      "outputs": [],
      "source": [
        "def run_10_node():\n",
        "\n",
        "  model0, qubit0 = build_model(theta_a, theta_b) #global\n",
        "  model1, qubit1 = build_model(theta_a, theta_b)\n",
        "  model2, qubit2 = build_model(theta_a, theta_b)\n",
        "  model3, qubit3 = build_model(theta_a, theta_b)\n",
        "  model4, qubit4 = build_model(theta_a, theta_b)\n",
        "  model5, qubit5 = build_model(theta_a, theta_b)\n",
        "  model6, qubit6 = build_model(theta_a, theta_b)\n",
        "  model7, qubit7 = build_model(theta_a, theta_b)\n",
        "  model8, qubit8 = build_model(theta_a, theta_b)\n",
        "  model9, qubit9 = build_model(theta_a, theta_b)\n",
        "  model10, qubit10 = build_model(theta_a, theta_b)\n",
        "\n",
        "  loss = []       # Loss value from each epoch\n",
        "  accuracy = []   # Accuracy % of each epoch\n",
        "\n",
        "  for round in range(iterations):\n",
        "\n",
        "    # Generate new local data for clients, D1, D2, D3, D4\n",
        "    qubit = cirq.GridQubit(0, 0)\n",
        "    q_data, labels, bloch_p1 = generate_dataset(qubit, theta_a, theta_b, num_samples) \n",
        "\n",
        "    q_data1 = q_data[:500]\n",
        "    q_data2 = q_data[500:1000]\n",
        "    q_data3 = q_data[1000:1500]\n",
        "    q_data4 = q_data[1500:2000]\n",
        "    q_data5 = q_data[2000:2500]\n",
        "    q_data6 = q_data[2500:3000]\n",
        "    q_data7 = q_data[3000:3500]\n",
        "    q_data8 = q_data[3500:4000]\n",
        "    q_data9 = q_data[4000:4500]\n",
        "    q_data10 = q_data[4500:5000]\n",
        "    \n",
        "    labels1 = labels[:500]\n",
        "    labels2 = labels[500:1000]\n",
        "    labels3 = labels[1000:1500]\n",
        "    labels4 = labels[1500:2000]\n",
        "    labels5 = labels[2000:2500] \n",
        "    labels6 = labels[2500:3000]\n",
        "    labels7 = labels[3000:3500]\n",
        "    labels8 = labels[3500:4000]\n",
        "    labels9 = labels[4000:4500]\n",
        "    labels10 = labels[4500:5000]\n",
        "\n",
        "    # Train clients with local data\n",
        "    history1 = model1.fit(x=q_data1, y=labels1, epochs=num_epochs, verbose=0)\n",
        "    history2 = model2.fit(x=q_data2, y=labels2, epochs=num_epochs, verbose=0)\n",
        "    history3 = model3.fit(x=q_data3, y=labels3, epochs=num_epochs, verbose=0)\n",
        "    history4 = model4.fit(x=q_data4, y=labels4, epochs=num_epochs, verbose=0)\n",
        "    history5 = model5.fit(x=q_data5, y=labels5, epochs=num_epochs, verbose=0)\n",
        "    history6 = model6.fit(x=q_data6, y=labels6, epochs=num_epochs, verbose=0)\n",
        "    history7 = model7.fit(x=q_data7, y=labels7, epochs=num_epochs, verbose=0)\n",
        "    history8 = model8.fit(x=q_data8, y=labels8, epochs=num_epochs, verbose=0)\n",
        "    history9 = model9.fit(x=q_data9, y=labels9, epochs=num_epochs, verbose=0)\n",
        "    history10 = model10.fit(x=q_data10, y=labels10, epochs=num_epochs, verbose=0)\n",
        "\n",
        "    weights0 = model0.get_weights()   # Extract global weights (parameters)\n",
        "    weights1 = model1.get_weights()   # Extract client weights (parameters)\n",
        "    weights2 = model2.get_weights()   # Extract client weights (parameters)\n",
        "    weights3 = model3.get_weights()   # Extract client weights (parameters)\n",
        "    weights4 = model4.get_weights()   # Extract client weights (parameters)\n",
        "    weights5 = model5.get_weights()   # Extract global weights (parameters)\n",
        "    weights6 = model6.get_weights()   # Extract client weights (parameters)\n",
        "    weights7 = model7.get_weights()   # Extract client weights (parameters)\n",
        "    weights8 = model8.get_weights()   # Extract client weights (parameters)\n",
        "    weights9 = model9.get_weights()   # Extract client weights (parameters)\n",
        "    weights10 = model10.get_weights()   # Extract client weights (parameters)\n",
        "\n",
        "    # Average weights\n",
        "    avg_weights = weights0\n",
        "    for j in range(len(weights0)):\n",
        "      avg_weights[j] = (weights0[j] + weights1[j] + weights2[j] + weights3[j] \n",
        "                        + weights4[j] + weights5[j] + weights6[j] + weights7[j] \n",
        "                        + weights8[j] + weights9[j] + weights10[j]) / 11\n",
        "\n",
        "    # Update weights\n",
        "    model0.set_weights(avg_weights)\n",
        "    model1.set_weights(avg_weights)\n",
        "    model2.set_weights(avg_weights)\n",
        "    model3.set_weights(avg_weights)\n",
        "    model4.set_weights(avg_weights)\n",
        "    model5.set_weights(avg_weights)\n",
        "    model6.set_weights(avg_weights)\n",
        "    model7.set_weights(avg_weights)\n",
        "    model8.set_weights(avg_weights)\n",
        "    model9.set_weights(avg_weights)\n",
        "    model10.set_weights(avg_weights)\n",
        "\n",
        "    # Test updated global model for metrics\n",
        "    qubit = cirq.GridQubit(0, 0)\n",
        "    test_data, test_labels, test_bloch_p = generate_dataset(qubit, theta_a, theta_b, 500)\n",
        "    test_results = model0.evaluate(test_data,test_labels, verbose=0)\n",
        "\n",
        "    loss.append(test_results[0])        # Save loss value of each round\n",
        "    accuracy.append(test_results[1])    # Save accuracy of each round\n",
        "\n",
        "  return loss, accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M8fLmLvMqs1m"
      },
      "source": [
        "## Run frameworks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RLeaElp2Ay6-"
      },
      "source": [
        "## 20-Node Framework\n",
        "Implement Binary Classification in a federated learning framework using 20 clients. Model 0 represents the global model. Models 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20 are clients. The parameters are averaged and used to update the global model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LUAXLgHZBhvx"
      },
      "outputs": [],
      "source": [
        "def run_20_node():\n",
        "\n",
        "  model0, qubit0 = build_model(theta_a, theta_b) #global\n",
        "  model1, qubit1 = build_model(theta_a, theta_b)\n",
        "  model2, qubit2 = build_model(theta_a, theta_b)\n",
        "  model3, qubit3 = build_model(theta_a, theta_b)\n",
        "  model4, qubit4 = build_model(theta_a, theta_b)\n",
        "  model5, qubit5 = build_model(theta_a, theta_b)\n",
        "  model6, qubit6 = build_model(theta_a, theta_b)\n",
        "  model7, qubit7 = build_model(theta_a, theta_b)\n",
        "  model8, qubit8 = build_model(theta_a, theta_b)\n",
        "  model9, qubit9 = build_model(theta_a, theta_b)\n",
        "  model10, qubit10 = build_model(theta_a, theta_b)\n",
        "  model11, qubit11 = build_model(theta_a, theta_b)\n",
        "  model12, qubit12 = build_model(theta_a, theta_b)\n",
        "  model13, qubit13 = build_model(theta_a, theta_b)\n",
        "  model14, qubit14 = build_model(theta_a, theta_b)\n",
        "  model15, qubit15 = build_model(theta_a, theta_b)\n",
        "  model16, qubit16 = build_model(theta_a, theta_b)\n",
        "  model17, qubit17 = build_model(theta_a, theta_b)\n",
        "  model18, qubit18 = build_model(theta_a, theta_b)\n",
        "  model19, qubit19 = build_model(theta_a, theta_b)\n",
        "  model20, qubit20 = build_model(theta_a, theta_b)\n",
        "\n",
        "  loss = []       # Loss value from each epoch\n",
        "  accuracy = []   # Accuracy % of each epoch\n",
        "\n",
        "  for round in range(iterations):\n",
        "\n",
        "    # Generate new local data for clients, D1, D2, D3, D4\n",
        "    qubit = cirq.GridQubit(0, 0)\n",
        "    q_data, labels, bloch_p = generate_dataset(qubit, theta_a, theta_b, num_samples) \n",
        "\n",
        "    q_data1 = q_data[:250]\n",
        "    q_data2 = q_data[250:500]\n",
        "    q_data3 = q_data[500:750]\n",
        "    q_data4 = q_data[750:1000]\n",
        "    q_data5 = q_data[1000:1250]\n",
        "    q_data6 = q_data[1250:1500]\n",
        "    q_data7 = q_data[1500:1750]\n",
        "    q_data8 = q_data[1750:2000]\n",
        "    q_data9 = q_data[2000:2250]\n",
        "    q_data10 = q_data[2250:2500]\n",
        "    q_data11 = q_data[2500:2750]\n",
        "    q_data12 = q_data[2750:3000]\n",
        "    q_data13 = q_data[3000:3250]\n",
        "    q_data14 = q_data[3250:3500]\n",
        "    q_data15 = q_data[3500:3750]\n",
        "    q_data16 = q_data[3750:4000]\n",
        "    q_data17 = q_data[4000:4250]\n",
        "    q_data18 = q_data[4250:4500]\n",
        "    q_data19 = q_data[4500:4750]\n",
        "    q_data20 = q_data[4750:5000]\n",
        "    \n",
        "    labels1 = labels[:250]\n",
        "    labels2 = labels[250:500]\n",
        "    labels3 = labels[500:750]\n",
        "    labels4 = labels[750:1000]\n",
        "    labels5 = labels[1000:1250] \n",
        "    labels6 = labels[1250:1500]\n",
        "    labels7 = labels[1500:1750]\n",
        "    labels8 = labels[1750:2000]\n",
        "    labels9 = labels[2000:2250]\n",
        "    labels10 = labels[2250:2500]\n",
        "    labels11 = labels[2500:2750]\n",
        "    labels12 = labels[2750:3000]\n",
        "    labels13 = labels[3000:3250]\n",
        "    labels14 = labels[3250:3500]\n",
        "    labels15 = labels[3500:3750] \n",
        "    labels16 = labels[3750:4000]\n",
        "    labels17 = labels[4000:4250]\n",
        "    labels18 = labels[4250:4500]\n",
        "    labels19 = labels[4500:4750]\n",
        "    labels20 = labels[4750:5000]\n",
        "\n",
        "    # Train clients with local data\n",
        "    history1 = model1.fit(x=q_data1, y=labels1, epochs=num_epochs, verbose=0)\n",
        "    history2 = model2.fit(x=q_data2, y=labels2, epochs=num_epochs, verbose=0)\n",
        "    history3 = model3.fit(x=q_data3, y=labels3, epochs=num_epochs, verbose=0)\n",
        "    history4 = model4.fit(x=q_data4, y=labels4, epochs=num_epochs, verbose=0)\n",
        "    history5 = model5.fit(x=q_data5, y=labels5, epochs=num_epochs, verbose=0)\n",
        "    history6 = model6.fit(x=q_data6, y=labels6, epochs=num_epochs, verbose=0)\n",
        "    history7 = model7.fit(x=q_data7, y=labels7, epochs=num_epochs, verbose=0)\n",
        "    history8 = model8.fit(x=q_data8, y=labels8, epochs=num_epochs, verbose=0)\n",
        "    history9 = model9.fit(x=q_data9, y=labels9, epochs=num_epochs, verbose=0)\n",
        "    history10 = model10.fit(x=q_data10, y=labels10, epochs=num_epochs, verbose=0)\n",
        "    history11 = model11.fit(x=q_data11, y=labels11, epochs=num_epochs, verbose=0)\n",
        "    history12 = model12.fit(x=q_data12, y=labels12, epochs=num_epochs, verbose=0)\n",
        "    history13 = model13.fit(x=q_data13, y=labels13, epochs=num_epochs, verbose=0)\n",
        "    history14 = model14.fit(x=q_data14, y=labels14, epochs=num_epochs, verbose=0)\n",
        "    history15 = model15.fit(x=q_data15, y=labels15, epochs=num_epochs, verbose=0)\n",
        "    history16 = model16.fit(x=q_data16, y=labels16, epochs=num_epochs, verbose=0)\n",
        "    history17 = model17.fit(x=q_data17, y=labels17, epochs=num_epochs, verbose=0)\n",
        "    history18 = model18.fit(x=q_data18, y=labels18, epochs=num_epochs, verbose=0)\n",
        "    history19 = model19.fit(x=q_data19, y=labels19, epochs=num_epochs, verbose=0)\n",
        "    history20 = model20.fit(x=q_data20, y=labels20, epochs=num_epochs, verbose=0)\n",
        "\n",
        "    # Extract global weights (parameters)\n",
        "    weights0 = model0.get_weights()   \n",
        "    weights1 = model1.get_weights()  \n",
        "    weights2 = model2.get_weights()  \n",
        "    weights3 = model3.get_weights()  \n",
        "    weights4 = model4.get_weights()   \n",
        "    weights5 = model5.get_weights() \n",
        "    weights6 = model6.get_weights()   \n",
        "    weights7 = model7.get_weights()   \n",
        "    weights8 = model8.get_weights()  \n",
        "    weights9 = model9.get_weights()  \n",
        "    weights10 = model10.get_weights()  \n",
        "    weights11 = model11.get_weights()  \n",
        "    weights12 = model12.get_weights()  \n",
        "    weights13 = model13.get_weights()  \n",
        "    weights14 = model14.get_weights()   \n",
        "    weights15 = model15.get_weights() \n",
        "    weights16 = model16.get_weights()   \n",
        "    weights17 = model17.get_weights()   \n",
        "    weights18 = model18.get_weights()  \n",
        "    weights19 = model19.get_weights()  \n",
        "    weights20 = model20.get_weights()   \n",
        "\n",
        "    # Average weights\n",
        "    avg_weights = weights0\n",
        "    for j in range(len(weights0)):\n",
        "      avg_weights[j] = (weights0[j] + weights1[j] + weights2[j] + weights3[j] \n",
        "                        + weights4[j] + weights5[j] + weights6[j] + weights7[j] \n",
        "                        + weights8[j] + weights9[j] + weights10[j]\n",
        "                        + weights11[j] + weights12[j] + weights13[j] \n",
        "                        + weights14[j] + weights15[j] + weights16[j] + weights17[j] \n",
        "                        + weights18[j] + weights19[j] + weights20[j]) / 21\n",
        "\n",
        "    # Update weights\n",
        "    model0.set_weights(avg_weights)\n",
        "    model1.set_weights(avg_weights)\n",
        "    model2.set_weights(avg_weights)\n",
        "    model3.set_weights(avg_weights)\n",
        "    model4.set_weights(avg_weights)\n",
        "    model5.set_weights(avg_weights)\n",
        "    model6.set_weights(avg_weights)\n",
        "    model7.set_weights(avg_weights)\n",
        "    model8.set_weights(avg_weights)\n",
        "    model9.set_weights(avg_weights)\n",
        "    model10.set_weights(avg_weights)\n",
        "    model11.set_weights(avg_weights)\n",
        "    model12.set_weights(avg_weights)\n",
        "    model13.set_weights(avg_weights)\n",
        "    model14.set_weights(avg_weights)\n",
        "    model15.set_weights(avg_weights)\n",
        "    model16.set_weights(avg_weights)\n",
        "    model17.set_weights(avg_weights)\n",
        "    model18.set_weights(avg_weights)\n",
        "    model19.set_weights(avg_weights)\n",
        "    model20.set_weights(avg_weights)\n",
        "\n",
        "    # Test updated global model for metrics\n",
        "    qubit = cirq.GridQubit(0, 0)\n",
        "    test_data, test_labels, test_bloch_p = generate_dataset(qubit, theta_a, theta_b, 500)\n",
        "    test_results = model0.evaluate(test_data,test_labels, verbose=0)\n",
        "\n",
        "    loss.append(test_results[0])        # Save loss value of each round\n",
        "    accuracy.append(test_results[1])    # Save accuracy of each round\n",
        "\n",
        "  return loss, accuracy"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss1_1, accuracy1_1 = run_1_node()\n",
        "loss1_2, accuracy1_2 = run_1_node()\n",
        "loss1_3, accuracy1_3 = run_1_node()\n",
        "loss1_4, accuracy1_4 = run_1_node()\n",
        "loss1_5, accuracy1_5 = run_1_node()"
      ],
      "metadata": {
        "id": "LX1tQGgrDviN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss1 = []\n",
        "accuracy1 = []\n",
        "\n",
        "# Calculate average loss\n",
        "for i in range(len(loss1_1)):\n",
        "  loss1.append((loss1_1[i] + loss1_2[i] + loss1_3[i] + loss1_4[i] + loss1_5[i]) / 5)\n",
        "\n",
        "print('loss1_1 :', loss1_1)\n",
        "print('loss1_2 :', loss1_2)\n",
        "print('loss1_3 :', loss1_3)\n",
        "print('loss1_4 :', loss1_4)\n",
        "print('loss1_5 :', loss1_5)\n",
        "print('loss1 : ', loss1)\n",
        "\n",
        "\n",
        "# Calculate average accuracy\n",
        "for i in range(len(accuracy1_1)):\n",
        "  accuracy1.append((accuracy1_1[i] + accuracy1_2[i] + accuracy1_3[i] + accuracy1_4[i] + accuracy1_5[i]) / 5)\n",
        "\n",
        "print('accuracy1_1 :', accuracy1_1)\n",
        "print('accuracy1_2 :', accuracy1_2)\n",
        "print('accuracy1_3 :', accuracy1_3)\n",
        "print('accuracy1_4 :', accuracy1_4)\n",
        "print('accuracy1_5 :', accuracy1_5)\n",
        "print('accuracy1 : ', accuracy1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ycd9FHfLHOyg",
        "outputId": "390b39b0-b4cb-4d34-f511-887612ce996c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss1_1 : [2.15166974067688, 0.10122592002153397, 0.0013662538258358836, 0.0001621867559151724, 5.293066715239547e-05, 2.588459028629586e-05, 1.4876136447128374e-05, 9.11751703824848e-06, 5.991897069179686e-06, 4.2743454287119675e-06]\n",
            "loss1_2 : [0.06663788110017776, 0.00395142612978816, 0.0005666977376677096, 0.00018395567894913256, 8.329839329235256e-05, 4.665280721383169e-05, 2.727819446590729e-05, 1.7137443137471564e-05, 1.0656955055310391e-05, 6.747438874299405e-06]\n",
            "loss1_3 : [0.2619248330593109, 0.007000266574323177, 0.0006936350837349892, 0.00013865591608919203, 5.153041274752468e-05, 2.584010871942155e-05, 1.5552739569102414e-05, 9.896883966575842e-06, 5.799491646030219e-06, 3.752454176719766e-06]\n",
            "loss1_4 : [1.5702266693115234, 0.08411718904972076, 0.0016238261014223099, 0.0001997999643208459, 6.14240561844781e-05, 2.9765922590740956e-05, 1.8538597942097113e-05, 1.132546276494395e-05, 6.799404218327254e-06, 4.474136403587181e-06]\n",
            "loss1_5 : [0.02150203287601471, 0.0018778468947857618, 0.0004601101391017437, 0.00015680510841775686, 7.857660239096731e-05, 4.157247531111352e-05, 2.4798546292004175e-05, 1.6721203792258166e-05, 1.0571600796538405e-05, 6.648730959568638e-06]\n",
            "loss1 :  [0.8143922314047813, 0.039634529734030366, 0.0009421045775525272, 0.00016828068473841994, 6.555202635354363e-05, 3.3943180824280716e-05, 2.0208842943247874e-05, 1.2839702139899601e-05, 7.963869757077191e-06, 5.1794211685773915e-06]\n",
            "accuracy1_1 : [0.21400000154972076, 0.9739999771118164, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n",
            "accuracy1_2 : [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n",
            "accuracy1_3 : [0.9200000166893005, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n",
            "accuracy1_4 : [0.28200000524520874, 0.9879999756813049, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n",
            "accuracy1_5 : [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n",
            "accuracy1 :  [0.683200004696846, 0.9923999905586243, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss2_1, accuracy2_1 = run_2_node()\n",
        "loss2_2, accuracy2_2 = run_2_node()\n",
        "loss2_3, accuracy2_3 = run_2_node()\n",
        "loss2_4, accuracy2_4 = run_2_node()\n",
        "loss2_5, accuracy2_5 = run_2_node()"
      ],
      "metadata": {
        "id": "h0eNpHd2JYcV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss2 = []\n",
        "accuracy2 = []\n",
        "\n",
        "# Calculate average loss\n",
        "for i in range(len(loss2_1)):\n",
        "  loss2.append((loss2_1[i] + loss2_2[i] + loss2_3[i] + loss2_4[i] + loss2_5[i]) / 5)\n",
        "\n",
        "print('loss2_1 :', loss2_1)\n",
        "print('loss2_2 :', loss2_2)\n",
        "print('loss2_3 :', loss2_3)\n",
        "print('loss2_4 :', loss2_4)\n",
        "print('loss2_5 :', loss2_5)\n",
        "print('loss2 : ', loss2)\n",
        "\n",
        "\n",
        "# Calculate average accuracy\n",
        "for i in range(len(accuracy2_1)):\n",
        "  accuracy2.append((accuracy2_1[i] + accuracy2_2[i] + accuracy2_3[i] + accuracy2_4[i] + accuracy2_5[i]) / 5)\n",
        "\n",
        "print('accuracy2_1 :', accuracy2_1)\n",
        "print('accuracy2_2 :', accuracy2_2)\n",
        "print('accuracy2_3 :', accuracy2_3)\n",
        "print('accuracy2_4 :', accuracy2_4)\n",
        "print('accuracy2_5 :', accuracy2_5)\n",
        "print('accuracy2 : ', accuracy2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WFG2IUyuJuAI",
        "outputId": "79624493-eaa4-49d9-9340-5c70035e21ae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss2_1 : [0.6747549176216125, 0.01194025482982397, 0.0010815371060743928, 0.0003824167652055621, 0.00021161383483558893, 0.00013257841055747122, 9.779535321285948e-05, 6.539483729284257e-05, 5.146957118995488e-05, 3.577920142561197e-05]\n",
            "loss2_2 : [4.671791076660156, 0.04101314768195152, 0.000896874931640923, 0.00029872028972022235, 0.0001666540774749592, 0.00011692709813360125, 8.820309449220076e-05, 6.45335967419669e-05, 4.986313433619216e-05, 4.0235008782474324e-05]\n",
            "loss2_3 : [0.6473646759986877, 0.005730850622057915, 0.0005406226264312863, 0.0002220857422798872, 0.00012185538798803464, 8.204281039070338e-05, 5.324427911546081e-05, 3.8541715184692293e-05, 2.8433980332920328e-05, 2.0159597625024617e-05]\n",
            "loss2_4 : [0.21470922231674194, 0.002951283473521471, 0.0005817413330078125, 0.0002637657162267715, 0.000150583335198462, 0.00010177010699408129, 7.507140253437683e-05, 5.2826286264462397e-05, 4.1036746551981196e-05, 3.144308357150294e-05]\n",
            "loss2_5 : [0.056706950068473816, 0.0015516186831519008, 0.0003885338664986193, 0.00016439378669019789, 0.0001023566655931063, 6.881717126816511e-05, 4.751030428451486e-05, 3.353350257384591e-05, 2.5939030820154585e-05, 2.0413730453583412e-05]\n",
            "loss2 :  [1.2530653685331345, 0.012637431058101356, 0.0006978619727306068, 0.0002662764600245282, 0.0001506126602180302, 0.00010042711946880444, 7.236488672788255e-05, 5.0965987611562014e-05, 3.934849264624063e-05, 2.9606124371639453e-05]\n",
            "accuracy2_1 : [0.7459999918937683, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n",
            "accuracy2_2 : [0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n",
            "accuracy2_3 : [0.8539999723434448, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n",
            "accuracy2_4 : [0.9139999747276306, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n",
            "accuracy2_5 : [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n",
            "accuracy2 :  [0.7027999877929687, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss5_1, accuracy5_1 = run_5_node()\n",
        "loss5_2, accuracy5_2 = run_5_node()\n",
        "loss5_3, accuracy5_3 = run_5_node()\n",
        "loss5_4, accuracy5_4 = run_5_node()\n",
        "loss5_5, accuracy5_5 = run_5_node()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 381
        },
        "id": "q9tDC0HuJZEU",
        "outputId": "ba57c0db-94b4-48f9-a671-cd6f1f5485d4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-8c219191229d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mloss5_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy5_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_5_node\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mloss5_2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy5_2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_5_node\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mloss5_3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy5_3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_5_node\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mloss5_4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy5_4\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_5_node\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mloss5_5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy5_5\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_5_node\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-10-a3085c30f303>\u001b[0m in \u001b[0;36mrun_5_node\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;31m# Generate new local data for clients, D1, D2, D3, D4\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mqubit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcirq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGridQubit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0mq_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mqubit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtheta_a\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtheta_b\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_samples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mq_data1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mq_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-55e3cb8c19de>\u001b[0m in \u001b[0;36mgenerate_dataset\u001b[0;34m(qubit, theta_a, theta_b, num_samples)\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mbloch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mangle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspread_x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0mbloch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mangle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcos\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspread_x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mtfq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbloch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/autograph/impl/api.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    640\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    641\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mag_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mControlStatusCtx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mag_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mStatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDISABLED\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 642\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    643\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    644\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mismethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow_quantum/python/util.py\u001b[0m in \u001b[0;36mconvert_to_tensor\u001b[0;34m(items_to_convert, deterministic_proto_serialize)\u001b[0m\n\u001b[1;32m    331\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    332\u001b[0m     \u001b[0;31m# This will catch impossible dimensions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 333\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecur\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitems_to_convert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    334\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    335\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow_quantum/python/util.py\u001b[0m in \u001b[0;36mrecur\u001b[0;34m(items_to_convert, curr_type)\u001b[0m\n\u001b[1;32m    322\u001b[0m                 \u001b[0mcurr_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcirq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCircuit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m                 tensored_items.append(\n\u001b[0;32m--> 324\u001b[0;31m                     serializer.serialize_circuit(item).SerializeToString(\n\u001b[0m\u001b[1;32m    325\u001b[0m                         deterministic=deterministic_proto_serialize))\n\u001b[1;32m    326\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow_quantum/core/serialize/serializer.py\u001b[0m in \u001b[0;36mserialize_circuit\u001b[0;34m(circuit_inp)\u001b[0m\n\u001b[1;32m    879\u001b[0m             new_ops[op.qubits] if op.qubits in new_ops else op for op in moment)\n\u001b[1;32m    880\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 881\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mSERIALIZER\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mserialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcircuit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    882\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    883\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow_quantum/core/serialize/serializable_gate_set.py\u001b[0m in \u001b[0;36mserialize\u001b[0;34m(self, program, msg, arg_function_language)\u001b[0m\n\u001b[1;32m    130\u001b[0m             self._serialize_circuit(program,\n\u001b[1;32m    131\u001b[0m                                     \u001b[0mmsg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcircuit\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m                                     arg_function_language=arg_function_language)\n\u001b[0m\u001b[1;32m    133\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0marg_function_language\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m                 arg_function_language = (_infer_function_language_from_circuit(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow_quantum/core/serialize/serializable_gate_set.py\u001b[0m in \u001b[0;36m_serialize_circuit\u001b[0;34m(self, circuit, msg, arg_function_language)\u001b[0m\n\u001b[1;32m    229\u001b[0m                 self.serialize_op(op,\n\u001b[1;32m    230\u001b[0m                                   \u001b[0mmoment_proto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moperations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 231\u001b[0;31m                                   arg_function_language=arg_function_language)\n\u001b[0m\u001b[1;32m    232\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m     def _deserialize_circuit(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow_quantum/core/serialize/serializable_gate_set.py\u001b[0m in \u001b[0;36mserialize_op\u001b[0;34m(self, op, msg, arg_function_language)\u001b[0m\n\u001b[1;32m    163\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mserializer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mserializers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mgate_type_mro\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m                     proto_msg = serializer.to_proto(\n\u001b[0;32m--> 165\u001b[0;31m                         op, msg, arg_function_language=arg_function_language)\n\u001b[0m\u001b[1;32m    166\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mproto_msg\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m                         \u001b[0;32mreturn\u001b[0m \u001b[0mproto_msg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow_quantum/core/serialize/op_serializer.py\u001b[0m in \u001b[0;36mto_proto\u001b[0;34m(self, op, msg, arg_function_language)\u001b[0m\n\u001b[1;32m    201\u001b[0m             \u001b[0mmsg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqubits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mqubit_to_proto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mqubit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 203\u001b[0;31m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_value_from_gate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    204\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mnot\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefault\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefault\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m                 _arg_to_proto(value,\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow_quantum/core/serialize/op_serializer.py\u001b[0m in \u001b[0;36m_value_from_gate\u001b[0;34m(self, op, arg)\u001b[0m\n\u001b[1;32m    211\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m         \u001b[0mop_getter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mop_getter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 213\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop_getter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    214\u001b[0m             \u001b[0mgate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop_getter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss5 = []\n",
        "accuracy5 = []\n",
        "\n",
        "# Calculate average loss\n",
        "for i in range(len(loss5_1)):\n",
        "  loss5.append((loss5_1[i] + loss5_2[i] + loss5_3[i] + loss5_4[i] + loss5_5[i]) / 5)\n",
        "\n",
        "print('loss5_1 :', loss5_1)\n",
        "print('loss5_2 :', loss5_2)\n",
        "print('loss5_3 :', loss5_3)\n",
        "print('loss5_4 :', loss5_4)\n",
        "print('loss5_5 :', loss5_5)\n",
        "print('loss5 : ', loss5)\n",
        "\n",
        "\n",
        "# Calculate average accuracy\n",
        "for i in range(len(accuracy5_1)):\n",
        "  accuracy5.append((accuracy5_1[i] + accuracy5_2[i] + accuracy5_3[i] + accuracy5_4[i] + accuracy5_5[i]) / 5)\n",
        "\n",
        "print('accuracy5_1 :', accuracy5_1)\n",
        "print('accuracy5_2 :', accuracy5_2)\n",
        "print('accuracy5_3 :', accuracy5_3)\n",
        "print('accuracy5_4 :', accuracy5_4)\n",
        "print('accuracy5_5 :', accuracy5_5)\n",
        "print('accuracy5 : ', accuracy5)"
      ],
      "metadata": {
        "id": "SHiYHkQpJ_gM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss10_1, accuracy10_1 = run_10_node()\n",
        "loss10_2, accuracy10_2 = run_10_node()\n",
        "loss10_3, accuracy10_3 = run_10_node()\n",
        "loss10_4, accuracy10_4 = run_10_node()\n",
        "loss10_5, accuracy10_5 = run_10_node()"
      ],
      "metadata": {
        "id": "gKNQnXRbJi9r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss10 = []\n",
        "accuracy10 = []\n",
        "\n",
        "# Calculate average loss\n",
        "for i in range(len(loss10_1)):\n",
        "  loss10.append((loss10_1[i] + loss10_2[i] + loss10_3[i] + loss10_4[i] + loss10_5[i]) / 5)\n",
        "\n",
        "print('loss10_1 :', loss10_1)\n",
        "print('loss10_2 :', loss10_2)\n",
        "print('loss10_3 :', loss10_3)\n",
        "print('loss10_4 :', loss10_4)\n",
        "print('loss10_5 :', loss10_5)\n",
        "print('loss10 : ', loss10)\n",
        "\n",
        "\n",
        "# Calculate average accuracy\n",
        "for i in range(len(accuracy10_1)):\n",
        "  accuracy10.append((accuracy10_1[i] + accuracy10_2[i] + accuracy10_3[i] + accuracy10_4[i] + accuracy10_5[i]) / 5)\n",
        "\n",
        "print('accuracy10_1 :', accuracy10_1)\n",
        "print('accuracy10_2 :', accuracy10_2)\n",
        "print('accuracy10_3 :', accuracy10_3)\n",
        "print('accuracy10_4 :', accuracy10_4)\n",
        "print('accuracy10_5 :', accuracy10_5)\n",
        "print('accuracy10 : ', accuracy10)"
      ],
      "metadata": {
        "id": "aZQpwDUtKaPG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss20_1, accuracy20_1 = run_20_node()\n",
        "loss20_2, accuracy20_2 = run_20_node()\n",
        "loss20_3, accuracy20_3 = run_20_node()\n",
        "loss20_4, accuracy20_4 = run_20_node()\n",
        "loss20_5, accuracy20_5 = run_20_node()"
      ],
      "metadata": {
        "id": "_KQvu0uzJi1E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss20 = []\n",
        "accuracy20 = []\n",
        "\n",
        "# Calculate average loss\n",
        "for i in range(len(loss20_1)):\n",
        "  loss20.append((loss20_1[i] + loss20_2[i] + loss20_3[i] + loss20_4[i] + loss20_5[i]) / 5)\n",
        "\n",
        "print('loss20_1 :', loss20_1)\n",
        "print('loss20_2 :', loss20_2)\n",
        "print('loss20_3 :', loss20_3)\n",
        "print('loss20_4 :', loss20_4)\n",
        "print('loss20_5 :', loss20_5)\n",
        "print('loss20 : ', loss20)\n",
        "\n",
        "\n",
        "# Calculate average accuracy\n",
        "for i in range(len(accuracy20_1)):\n",
        "  accuracy20.append((accuracy20_1[i] + accuracy20_2[i] + accuracy20_3[i] + accuracy20_4[i] + accuracy20_5[i]) / 5)\n",
        "\n",
        "print('accuracy20_1 :', accuracy20_1)\n",
        "print('accuracy20_2 :', accuracy20_2)\n",
        "print('accuracy20_3 :', accuracy20_3)\n",
        "print('accuracy20_4 :', accuracy20_4)\n",
        "print('accuracy20_5 :', accuracy20_5)\n",
        "print('accuracy20 : ', accuracy20)"
      ],
      "metadata": {
        "id": "HnCXoiDlK9Bl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eqh3YLYhthvB"
      },
      "outputs": [],
      "source": [
        "plt.plot(loss1, label=\"1 devices\")\n",
        "plt.plot(loss2, label=\"2 devices\")\n",
        "plt.plot(loss5, label=\"5 devices\")\n",
        "plt.plot(loss10, label=\"10 devices\")\n",
        "plt.plot(loss20, label=\"20 devices\")\n",
        "plt.xlabel(\"iterations\")\n",
        "plt.ylabel(\"loss value\")\n",
        "plt.legend()\n",
        "plt.title(\"Loss on training data\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0P7og3t2tnpW"
      },
      "outputs": [],
      "source": [
        "plt.plot(accuracy1, label=\"1 device\")\n",
        "plt.plot(accuracy2, label=\"2 devices\")\n",
        "plt.plot(accuracy5, label=\"5 devices\")\n",
        "plt.plot(accuracy10, label=\"10 devices\")\n",
        "plt.plot(accuracy20, label=\"20 devices\")\n",
        "plt.xlabel(\"iterations\")\n",
        "plt.ylabel(\"accuracy\")\n",
        "plt.legend()\n",
        "plt.title(\"accuracy on training data\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CdjgiKEOeTud"
      },
      "outputs": [],
      "source": [
        "print(\"Loss 1 :\", loss1)\n",
        "print(\"Loss 2 :\", loss2)\n",
        "print(\"Loss 5 :\", loss5)\n",
        "print(\"Loss 10 :\", loss10)\n",
        "print(\"Loss 20:\", loss20)\n",
        "print(\"-\"*10)\n",
        "print(\"Accuracy 1 : \", accuracy1)\n",
        "print(\"Accuracy 2 : \", accuracy2)\n",
        "print(\"Accuracy 5 : \", accuracy5)\n",
        "print(\"Accuracy 10 : \", accuracy10)\n",
        "print(\"Accuracy 20: \", accuracy20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kV2-_gbKdYCd"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Experiment2-TFQ_Example_BinaryClassifier.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}